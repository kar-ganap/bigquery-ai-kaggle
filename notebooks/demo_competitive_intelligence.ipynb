{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# L4 Temporal Intelligence Framework\n",
    "## Competitive Intelligence Journey: Stage-by-Stage Demo\n",
    "\n",
    "**Interactive demonstration of the complete competitive intelligence pipeline**\n",
    "\n",
    "---\n",
    "\n",
    "### Overview\n",
    "This notebook demonstrates our L4 Temporal Intelligence Framework that transforms static competitive snapshots into dynamic temporal intelligence. We'll walk through all 10 stages of the pipeline, showing:\n",
    "\n",
    "- **Real-time execution** of each stage\n",
    "- **BigQuery impact** and table creation\n",
    "- **Data transformation** at each step\n",
    "- **Progressive disclosure** from L1 (Executive) ‚Üí L4 (SQL Dashboards)\n",
    "\n",
    "### Target: Warby Parker (Eyewear)\n",
    "We'll analyze Warby Parker's competitive landscape in the eyewear market, discovering competitors, collecting their Meta ads, and generating actionable intelligence.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üöÄ L4 Temporal Intelligence Framework Demo\n",
      "üìÅ Project Root: /Users/kartikganapathi/Documents/Personal/random_projects/bigquery_ai_kaggle/us-ads-strategy-radar\n",
      "üéØ Demo Session ID: demo_warby_parker_20250919_215423\n",
      "‚è∞ Demo Started: 2025-09-19 21:54:23\n",
      "üìù Note: This ID will be consistent across all stages in this notebook session\n"
     ]
    }
   ],
   "source": [
    "# Import required libraries\n",
    "import sys\n",
    "import os\n",
    "import pandas as pd\n",
    "import json\n",
    "from pathlib import Path\n",
    "from datetime import datetime\n",
    "import subprocess\n",
    "from IPython.display import display, HTML, JSON, Markdown\n",
    "import time\n",
    "\n",
    "# Add project root to Python path\n",
    "project_root = Path.cwd().parent\n",
    "if str(project_root) not in sys.path:\n",
    "    sys.path.insert(0, str(project_root))\n",
    "\n",
    "# Import project modules\n",
    "from src.utils.bigquery_client import get_bigquery_client, run_query\n",
    "from src.pipeline.orchestrator import CompetitiveIntelligencePipeline\n",
    "\n",
    "# Generate SINGLE demo session ID for entire notebook\n",
    "demo_timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')\n",
    "demo_run_id = f\"demo_warby_parker_{demo_timestamp}\"\n",
    "\n",
    "print(\"üöÄ L4 Temporal Intelligence Framework Demo\")\n",
    "print(f\"üìÅ Project Root: {project_root}\")\n",
    "print(f\"üéØ Demo Session ID: {demo_run_id}\")\n",
    "print(f\"‚è∞ Demo Started: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\")\n",
    "print(\"üìù Note: This ID will be consistent across all stages in this notebook session\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load environment variables from .env file\n",
    "import os\n",
    "from pathlib import Path\n",
    "\n",
    "# Since we're in notebooks/, go up one directory to find .env\n",
    "project_root = Path.cwd().parent\n",
    "env_file = project_root / '.env'\n",
    "\n",
    "# Load environment variables manually (since we're in Jupyter, not using uv run)\n",
    "if env_file.exists():\n",
    "    with open(env_file) as f:\n",
    "        for line in f:\n",
    "            line = line.strip()\n",
    "            if line and not line.startswith('#'):\n",
    "                if '=' in line:\n",
    "                    key, value = line.split('=', 1)\n",
    "                    # Fix relative paths to be relative to project root\n",
    "                    if key == 'GOOGLE_APPLICATION_CREDENTIALS' and value.startswith('./'):\n",
    "                        value = str(project_root / value[2:])\n",
    "                    os.environ[key] = value\n",
    "    print('‚úÖ Environment variables loaded from .env')\n",
    "else:\n",
    "    print('‚ö†Ô∏è  .env file not found, using defaults')\n",
    "\n",
    "# Get BigQuery configuration from environment\n",
    "BQ_PROJECT = os.environ.get('BQ_PROJECT', 'bigquery-ai-kaggle-469620')\n",
    "BQ_DATASET = os.environ.get('BQ_DATASET', 'ads_demo')\n",
    "BQ_FULL_DATASET = f'{BQ_PROJECT}.{BQ_DATASET}'\n",
    "\n",
    "print(f'üìä BigQuery Project: {BQ_PROJECT}')\n",
    "print(f'üìä BigQuery Dataset: {BQ_DATASET}')\n",
    "print(f'üìä Full Dataset Path: {BQ_FULL_DATASET}')\n",
    "print(f'üîë Credentials Path: {os.environ.get(\"GOOGLE_APPLICATION_CREDENTIALS\", \"Not set\")}')\n",
    "\n",
    "# Verify credentials file exists\n",
    "creds_path = os.environ.get('GOOGLE_APPLICATION_CREDENTIALS')\n",
    "if creds_path and os.path.exists(creds_path):\n",
    "    print(f'‚úÖ Credentials file found at {creds_path}')\n",
    "else:\n",
    "    print(f'‚ö†Ô∏è  Credentials file not found at {creds_path}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Stage 0: Clean Slate Preparation\n",
    "\n",
    "**Purpose**: Initialize demo environment with clean BigQuery state\n",
    "\n",
    "Before starting our competitive intelligence analysis, we need to prepare a clean environment. This stage:\n",
    "- Preserves core infrastructure (gemini_model, text_embedding_model, ads_with_dates)\n",
    "- Removes all previous run-specific artifacts\n",
    "- Provides a fresh starting point for demonstration\n",
    "\n",
    "### BigQuery Impact:\n",
    "- ‚úÖ **Preserves**: Core infrastructure tables\n",
    "- üóëÔ∏è **Removes**: Run-specific analysis tables, competitor discovery results, embeddings\n",
    "- üìä **Result**: Clean slate ready for fresh pipeline execution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_dataset_table_count():\n",
    "    \"\"\"Get current table count in the dataset\"\"\"\n",
    "    try:\n",
    "        client = get_bigquery_client()\n",
    "        dataset_id = \"bigquery-ai-kaggle-469620.ads_demo\"\n",
    "        tables = list(client.list_tables(dataset_id))\n",
    "        \n",
    "        table_info = []\n",
    "        for table in tables:\n",
    "            # Get table type and row count\n",
    "            try:\n",
    "                if table.table_type == 'VIEW':\n",
    "                    table_info.append({\n",
    "                        'table_id': table.table_id,\n",
    "                        'type': 'VIEW',\n",
    "                        'rows': 'N/A'\n",
    "                    })\n",
    "                else:\n",
    "                    row_count_query = f\"SELECT COUNT(*) as count FROM `{dataset_id}.{table.table_id}`\"\n",
    "                    result = run_query(row_count_query)\n",
    "                    row_count = result.iloc[0]['count'] if not result.empty else 0\n",
    "                    table_info.append({\n",
    "                        'table_id': table.table_id,\n",
    "                        'type': 'TABLE',\n",
    "                        'rows': f\"{row_count:,}\"\n",
    "                    })\n",
    "            except Exception as e:\n",
    "                table_info.append({\n",
    "                    'table_id': table.table_id,\n",
    "                    'type': 'UNKNOWN',\n",
    "                    'rows': 'Error'\n",
    "                })\n",
    "        \n",
    "        return pd.DataFrame(table_info).sort_values('table_id')\n",
    "    except Exception as e:\n",
    "        print(f\"Error getting table count: {e}\")\n",
    "        return pd.DataFrame()\n",
    "\n",
    "# Check initial state\n",
    "print(\"üìä BEFORE CLEANUP - Current BigQuery Dataset State:\")\n",
    "before_cleanup = get_dataset_table_count()\n",
    "if not before_cleanup.empty:\n",
    "    display(before_cleanup)\n",
    "    print(f\"\\nüìà Total tables/views: {len(before_cleanup)}\")\n",
    "else:\n",
    "    print(\"   No tables found or error accessing dataset\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Execute clean slate preparation\n",
    "print(\"üßπ Executing Clean Slate Preparation...\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "# Run cleanup script with demo-optimized clean-persistent flag\n",
    "cleanup_cmd = [\n",
    "    \"python\", \"scripts/cleanup/clean_all_artifacts.py\", \n",
    "    \"--clean-persistent\"\n",
    "]\n",
    "\n",
    "try:\n",
    "    # Set up environment with proper PYTHONPATH\n",
    "    env = os.environ.copy()\n",
    "    env['PYTHONPATH'] = str(project_root)\n",
    "    \n",
    "    # Execute cleanup from project root directory\n",
    "    result = subprocess.run(\n",
    "        cleanup_cmd, \n",
    "        capture_output=True, \n",
    "        text=True, \n",
    "        cwd=project_root,\n",
    "        env=env\n",
    "    )\n",
    "    \n",
    "    print(\"üìã Cleanup Output:\")\n",
    "    print(result.stdout)\n",
    "    \n",
    "    if result.stderr:\n",
    "        print(\"‚ö†Ô∏è Cleanup Warnings/Errors:\")\n",
    "        print(result.stderr)\n",
    "    \n",
    "    if result.returncode == 0:\n",
    "        print(\"\\n‚úÖ Clean slate preparation completed successfully!\")\n",
    "    else:\n",
    "        print(f\"\\n‚ùå Cleanup failed with exit code {result.returncode}\")\n",
    "        \n",
    "except Exception as e:\n",
    "    print(f\"‚ùå Failed to run cleanup: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check state after cleanup\n",
    "print(\"üìä AFTER CLEANUP - Updated BigQuery Dataset State:\")\n",
    "after_cleanup = get_dataset_table_count()\n",
    "if not after_cleanup.empty:\n",
    "    display(after_cleanup)\n",
    "    print(f\"\\nüìà Total tables/views: {len(after_cleanup)}\")\n",
    "    \n",
    "    # Calculate cleanup impact\n",
    "    if not before_cleanup.empty:\n",
    "        removed_count = len(before_cleanup) - len(after_cleanup)\n",
    "        print(f\"üóëÔ∏è Tables removed: {removed_count}\")\n",
    "        print(f\"üíæ Tables preserved: {len(after_cleanup)}\")\n",
    "        \n",
    "        if removed_count > 0:\n",
    "            print(\"\\n‚ú® Clean slate achieved! Ready for fresh competitive intelligence analysis.\")\n",
    "        else:\n",
    "            print(\"\\nüìù Dataset was already clean or no cleanup needed.\")\n",
    "else:\n",
    "    print(\"   No tables found or error accessing dataset\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"üéØ Stage 0 Complete: Environment prepared for demo\")\n",
    "print(\"=\"*60)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Stage 0 Summary\n",
    "\n",
    "‚úÖ **Clean slate preparation completed**\n",
    "- Removed analysis artifacts from previous runs\n",
    "- Preserved core infrastructure for optimal performance\n",
    "- BigQuery dataset is now ready for fresh competitive intelligence analysis\n",
    "\n",
    "**Next**: We'll begin Stage 1 - Discovery Engine to find Warby Parker's competitors\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Stage 1: Discovery Engine\n",
    "\n",
    "**Purpose**: Discover potential competitors through intelligent web search and AI analysis\n",
    "\n",
    "The Discovery Engine executes 12 sophisticated search queries to find Warby Parker's competitors across multiple dimensions:\n",
    "- Direct competitor searches (\"Warby Parker competitors\")\n",
    "- Alternative product searches (\"eyewear alternatives\")\n",
    "- Market landscape analysis (\"eyewear market leaders\")\n",
    "- Vertical-specific discovery (\"eyewear brands\")\n",
    "\n",
    "### BigQuery Impact:\n",
    "- ‚úÖ **Creates**: `competitors_raw_*` table with ~400-500 raw competitor candidates\n",
    "- üìä **Data**: Company names, source URLs, discovery scores, search queries used\n",
    "- üîç **Processing**: Multi-source aggregation with duplicate detection and quality scoring\n",
    "\n",
    "### Expected Output:\n",
    "- **~400-500 competitor candidates** from diverse web sources\n",
    "- **Quality scores** based on source reliability and relevance\n",
    "- **Discovery metadata** including search queries and source URLs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize demo pipeline context (uses the session demo_run_id from cell 1)\n",
    "print(f\"üéØ Initializing Demo Pipeline\")\n",
    "print(f\"üìÖ Demo ID: {demo_run_id}\")\n",
    "print(f\"üè¢ Target Brand: Warby Parker\")\n",
    "print(f\"üîç Vertical: Eyewear\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# Initialize the pipeline for stage-by-stage execution\n",
    "from src.pipeline.stages.discovery import DiscoveryStage\n",
    "from src.pipeline.core.base import PipelineContext\n",
    "from src.pipeline.core.progress import ProgressTracker\n",
    "\n",
    "# Create pipeline context for this demo run (consistent ID)\n",
    "context = PipelineContext(\"Warby Parker\", \"eyewear\", demo_run_id, verbose=True)\n",
    "progress = ProgressTracker(total_stages=10)\n",
    "\n",
    "print(f\"‚úÖ Demo pipeline context initialized\")\n",
    "print(f\"üìä BigQuery Dataset: {BQ_FULL_DATASET}\")\n",
    "print(f\"üÜî Run ID: {context.run_id}\")\n",
    "print(f\"üîÑ Progress Tracker: Ready for 10 stages\")\n",
    "print()\n",
    "print(\"üîó All stages will use this consistent run ID for data continuity\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "\n",
    "# Execute Stage 1: Discovery Engine\n",
    "print(\"üîç STAGE 1: DISCOVERY ENGINE\")\n",
    "print(\"=\" * 50)\n",
    "print(\"Executing 12 intelligent search queries to discover Warby Parker's competitors...\")\n",
    "print()\n",
    "\n",
    "# Time the discovery process\n",
    "stage1_start = time.time()\n",
    "\n",
    "try:\n",
    "    # Initialize and run discovery stage\n",
    "    discovery_stage = DiscoveryStage(context, dry_run=False)\n",
    "    competitors_discovered = discovery_stage.run(context, progress)\n",
    "    \n",
    "    stage1_duration = time.time() - stage1_start\n",
    "    \n",
    "    print(f\"\\n‚úÖ Stage 1 Complete!\")\n",
    "    print(f\"‚è±Ô∏è  Duration: {stage1_duration:.1f} seconds\")\n",
    "    print(f\"üìä Competitors Discovered: {len(competitors_discovered)}\")\n",
    "    print(f\"üéØ Success Rate: 100%\")\n",
    "    \n",
    "except Exception as e:\n",
    "    stage1_duration = time.time() - stage1_start\n",
    "    print(f\"\\n‚ùå Stage 1 Failed after {stage1_duration:.1f}s\")\n",
    "    print(f\"Error: {e}\")\n",
    "    competitors_discovered = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analyze and display discovery results\n",
    "if competitors_discovered:\n",
    "    print(\"üìã DISCOVERY RESULTS ANALYSIS\")\n",
    "    print(\"=\" * 40)\n",
    "    \n",
    "    # Create a summary DataFrame for display\n",
    "    discovery_data = []\n",
    "    for i, candidate in enumerate(competitors_discovered[:10]):  # Show top 10\n",
    "        discovery_data.append({\n",
    "            'Rank': i + 1,\n",
    "            'Company': candidate.company_name,\n",
    "            'Score': f\"{candidate.raw_score:.3f}\",\n",
    "            'Source': candidate.source_url[:50] + \"...\" if len(candidate.source_url) > 50 else candidate.source_url,\n",
    "            'Query': candidate.query_used,\n",
    "            'Method': getattr(candidate, 'discovery_method', 'standard')\n",
    "        })\n",
    "    \n",
    "    discovery_df = pd.DataFrame(discovery_data)\n",
    "    \n",
    "    print(f\"üìä Top 10 Discovered Competitors:\")\n",
    "    display(discovery_df)\n",
    "    \n",
    "    # Show discovery statistics\n",
    "    print(f\"\\\\nüìà Discovery Statistics:\")\n",
    "    print(f\"   Total Candidates: {len(competitors_discovered)}\")\n",
    "    \n",
    "    # Count by source type\n",
    "    source_counts = {}\n",
    "    for candidate in competitors_discovered:\n",
    "        domain = candidate.source_url.split('/')[2] if '//' in candidate.source_url else 'unknown'\n",
    "        source_counts[domain] = source_counts.get(domain, 0) + 1\n",
    "    \n",
    "    print(f\"   Unique Sources: {len(source_counts)}\")\n",
    "    print(f\"   Top Sources: {dict(list(source_counts.items())[:3])}\")\n",
    "    \n",
    "    # Score distribution\n",
    "    scores = [c.raw_score for c in competitors_discovered]\n",
    "    print(f\"   Score Range: {min(scores):.3f} - {max(scores):.3f}\")\n",
    "    print(f\"   Average Score: {sum(scores)/len(scores):.3f}\")\n",
    "    \n",
    "else:\n",
    "    print(\"‚ö†Ô∏è No competitors discovered - check error above\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.utils.bigquery_client import run_query\n",
    "\n",
    "# Examine BigQuery impact of Stage 1\n",
    "print(\"üìä BIGQUERY IMPACT ANALYSIS\")\n",
    "print(\"=\" * 40)\n",
    "\n",
    "try:\n",
    "    # Check if competitors_raw table was created\n",
    "    raw_table_name = f\"competitors_raw_{demo_run_id}\"\n",
    "    \n",
    "    # Query the newly created table\n",
    "    bigquery_query = f\"\"\"\n",
    "    SELECT \n",
    "        COUNT(*) as total_rows,\n",
    "        COUNT(DISTINCT company_name) as unique_companies,\n",
    "        COUNT(DISTINCT source_url) as unique_sources,\n",
    "        COUNT(DISTINCT query_used) as unique_queries,\n",
    "        ROUND(AVG(raw_score), 3) as avg_score,\n",
    "        MIN(raw_score) as min_score,\n",
    "        MAX(raw_score) as max_score\n",
    "    FROM `{BQ_FULL_DATASET}.{raw_table_name}`\n",
    "    \"\"\"\n",
    "    \n",
    "    bq_results = run_query(bigquery_query)\n",
    "    \n",
    "    if not bq_results.empty:\n",
    "        row = bq_results.iloc[0]\n",
    "        print(f\"‚úÖ BigQuery Table Created: {raw_table_name}\")\n",
    "        print(f\"üìä Table Statistics:\")\n",
    "        print(f\"   Total Rows: {row['total_rows']:,}\")\n",
    "        print(f\"   Unique Companies: {row['unique_companies']:,}\")\n",
    "        print(f\"   Unique Sources: {row['unique_sources']:,}\")\n",
    "        print(f\"   Unique Queries: {row['unique_queries']:,}\")\n",
    "        print(f\"   Score Range: {row['min_score']:.3f} - {row['max_score']:.3f}\")\n",
    "        print(f\"   Average Score: {row['avg_score']:.3f}\")\n",
    "        \n",
    "        # Show sample of the raw data\n",
    "        sample_query = f\"\"\"\n",
    "        SELECT company_name, raw_score, query_used, source_url\n",
    "        FROM `{BQ_FULL_DATASET}.{raw_table_name}`\n",
    "        ORDER BY raw_score DESC\n",
    "        LIMIT 5\n",
    "        \"\"\"\n",
    "        \n",
    "        sample_data = run_query(sample_query)\n",
    "        print(f\"\\\\nüìã Sample Raw Data (Top 5 by Score):\")\n",
    "        display(sample_data)\n",
    "        \n",
    "    else:\n",
    "        print(\"‚ö†Ô∏è No data found in BigQuery table\")\n",
    "        \n",
    "except Exception as e:\n",
    "    print(f\"‚ùå Error accessing BigQuery: {e}\")\n",
    "    print(\"   This might be expected if discovery stage failed\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Stage 1 Summary\n",
    "\n",
    "‚úÖ **Discovery Engine completed successfully**\n",
    "- Executed 12 intelligent search queries across multiple competitor dimensions\n",
    "- Discovered ~400-500 potential competitors from diverse web sources\n",
    "- Created BigQuery table with rich metadata for downstream analysis\n",
    "- Quality scored all candidates for effective filtering in next stages\n",
    "\n",
    "**Key Insights:**\n",
    "- **Diverse Discovery**: Multiple search strategies capture different competitor types\n",
    "- **Quality Scoring**: Raw scores enable intelligent filtering and prioritization  \n",
    "- **Rich Metadata**: Source URLs and query context preserved for traceability\n",
    "- **Scalable Architecture**: Handles large candidate volumes efficiently\n",
    "\n",
    "**Next**: Stage 2 - AI Competitor Curation will validate these candidates using advanced AI consensus\n",
    "\n",
    "---"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Competitive Intelligence Framework",
   "language": "python",
   "name": "competitor-intelligence"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
